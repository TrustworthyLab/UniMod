<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Primary Meta Tags -->
  <meta name="title" content="From Sparse Decisions to Dense Reasoning: A Multi-attribute Trajectory Paradigm for Multimodal Moderation">
  <meta name="description" content="We propose UniMod, a multi-attribute trajectory learning paradigm for multimodal moderation that transitions from sparse binary decisions to dense structured reasoning traces (evidence grounding, modality assessment, risk mapping, policy decision, and response generation). To support dense supervision, we develop a multi-head scalar reward model (UniRM) and optimization strategies that mitigate objective interference in multi-task learning. Experiments show UniMod achieves competitive textual moderation and a new multimodal state-of-the-art using less than 40% of the training data required by leading baselines.">
  <meta name="keywords" content="Multimodal Moderation, AI Safety, Multimodal Safety, VLMs, Reward Modeling, Trajectory Supervision, Multi-objective Optimization, Guard Models">
  <meta name="author" content="Tianle Gu, Kexin Huang, Lingyu Li, Ruilin Luo, Shiyang Huang, Zongqi Wang, Yujiu Yang, Yan Teng, Yingchun Wang">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">

  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Tsinghua University · Shanghai Artificial Intelligence Laboratory · Fudan University">
  <meta property="og:title" content="From Sparse Decisions to Dense Reasoning: A Multi-attribute Trajectory Paradigm for Multimodal Moderation">
  <meta property="og:description" content="UniMod reformulates multimodal moderation from sparse binary decisions to dense structured reasoning trajectories, supported by a multi-head scalar reward model (UniRM) for attribute-level supervision and optimized multi-task training dynamics.">
  <meta property="og:url" content="https://YOUR_DOMAIN.com/unimod">
  <meta property="og:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="From Sparse Decisions to Dense Reasoning — UniMod Project Page">
  <meta property="article:published_time" content="2026-01-28T00:00:00.000Z">
  <meta property="article:author" content="Tianle Gu">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="Multimodal Moderation">
  <meta property="article:tag" content="AI Safety">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE">
  <meta name="twitter:title" content="From Sparse Decisions to Dense Reasoning: UniMod for Multimodal Moderation">
  <meta name="twitter:description" content="UniMod: dense structured reasoning trajectories + UniRM attribute-level rewards for efficient, strong multimodal moderation.">
  <meta name="twitter:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="From Sparse Decisions to Dense Reasoning — UniMod Project Page">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="From Sparse Decisions to Dense Reasoning: A Multi-attribute Trajectory Paradigm for Multimodal Moderation">
  <meta name="citation_author" content="Gu, Tianle">
  <meta name="citation_author" content="Huang, Kexin">
  <meta name="citation_author" content="Li, Lingyu">
  <meta name="citation_author" content="Luo, Ruilin">
  <meta name="citation_author" content="Huang, Shiyang">
  <meta name="citation_author" content="Wang, Zongqi">
  <meta name="citation_author" content="Yang, Yujiu">
  <meta name="citation_author" content="Teng, Yan">
  <meta name="citation_author" content="Wang, Yingchun">
  <meta name="citation_publication_date" content="2026">
  <meta name="citation_pdf_url" content="https://YOUR_DOMAIN.com/static/pdfs/paper.pdf">

  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">

  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">

  <title>From Sparse Decisions to Dense Reasoning — UniMod</title>

  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">

  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">

  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
  </noscript>

  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">

  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>

  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "From Sparse Decisions to Dense Reasoning: A Multi-attribute Trajectory Paradigm for Multimodal Moderation",
    "description": "UniMod transitions multimodal moderation from sparse binary decisions to dense structured reasoning trajectories, supported by UniRM attribute-level rewards and optimized multi-task training dynamics.",
    "author": [
      {"@type": "Person", "name": "Tianle Gu", "affiliation": [{"@type": "Organization", "name": "Tsinghua University"}, {"@type": "Organization", "name": "Shanghai Artificial Intelligence Laboratory"}]},
      {"@type": "Person", "name": "Kexin Huang", "affiliation": [{"@type": "Organization", "name": "Fudan University"}]},
      {"@type": "Person", "name": "Lingyu Li", "affiliation": [{"@type": "Organization", "name": "Shanghai Artificial Intelligence Laboratory"}]},
      {"@type": "Person", "name": "Ruilin Luo", "affiliation": [{"@type": "Organization", "name": "Tsinghua University"}]},
      {"@type": "Person", "name": "Shiyang Huang", "affiliation": [{"@type": "Organization", "name": "Shanghai Artificial Intelligence Laboratory"}]},
      {"@type": "Person", "name": "Zongqi Wang", "affiliation": [{"@type": "Organization", "name": "Tsinghua University"}]},
      {"@type": "Person", "name": "Yujiu Yang", "affiliation": [{"@type": "Organization", "name": "Tsinghua University"}]},
      {"@type": "Person", "name": "Yan Teng", "affiliation": [{"@type": "Organization", "name": "Shanghai Artificial Intelligence Laboratory"}]},
      {"@type": "Person", "name": "Yingchun Wang", "affiliation": [{"@type": "Organization", "name": "Shanghai Artificial Intelligence Laboratory"}]}
    ],
    "datePublished": "2026-01-28",
    "publisher": {"@type": "Organization", "name": "arXiv"},
    "url": "https://YOUR_DOMAIN.com/unimod",
    "image": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
    "keywords": ["Multimodal Moderation", "AI Safety", "Reward Modeling", "Trajectory Supervision", "Multi-objective Optimization", "VLMs"],
    "abstract": "We propose UniMod, a multi-attribute trajectory learning paradigm for multimodal moderation that transitions from sparse binary decisions to dense structured reasoning traces (evidence grounding, modality assessment, risk mapping, policy decision, and response generation). To support dense supervision, we develop a multi-head scalar reward model (UniRM) and optimization strategies that mitigate objective interference in multi-task learning. Experiments show UniMod achieves competitive textual moderation and a new multimodal state-of-the-art using less than 40% of the training data required by leading baselines.",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {"@type": "WebPage", "@id": "https://YOUR_DOMAIN.com/unimod"},
    "about": [
      {"@type": "Thing", "name": "Multimodal Safety"},
      {"@type": "Thing", "name": "Content Moderation"}
    ]
  }
  </script>

  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "Tsinghua University · Shanghai Artificial Intelligence Laboratory · Fudan University",
    "url": "https://YOUR_INSTITUTION_WEBSITE.com",
    "logo": "https://YOUR_DOMAIN.com/static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
</head>

<body>

  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <!-- More Works Dropdown -->
  <div class="more-works-container">
    <button class="more-works-btn" onclick="toggleMoreWorks()" title="View Recommended Related Works">
      <i class="fas fa-flask"></i>
      More Works
      <i class="fas fa-chevron-down dropdown-arrow"></i>
    </button>
    <div class="more-works-dropdown" id="moreWorksDropdown">
      <div class="dropdown-header">
        <h4>Recommended Readings</h4>
        <button class="close-btn" onclick="toggleMoreWorks()">
          <i class="fas fa-times"></i>
        </button>
      </div>
      <div class="works-list">
        <!-- Optional: add your lab's other papers -->
        <a href="https://arxiv.org/abs/2406.07594" class="work-item" target="_blank" rel="noopener">
          <div class="work-info">
            <h5>MLLMGuard: A Multi-dimensional Safety Evaluation Suite for Multimodal Large Language Models</h5>
            <p>Introducing a multi-dimensional safety evaluation suite for MLLMs, including a bilingual image-text evaluation dataset, inference utilities, and a set of lightweight evaluators.</p>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
      </div>
    </div>
  </div>

  <main id="main-content">
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">
                From Sparse Decisions to Dense Reasoning: A Multi-attribute Trajectory Paradigm for Multimodal Moderation
              </h1>

              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=wlW9_7QAAAAJ" target="_blank" rel="noopener">Tianle Gu<sup>1,2</sup></a>,
                </span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=JDcYYZ4AAAAJ" target="_blank" rel="noopener">Kexin Huang<sup>3</sup></a>,
                </span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=WCF6bV8AAAAJ" target="_blank" rel="noopener">Lingyu Li<sup>2</sup></a>,
                </span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=dNCLGEEAAAAJ" aria-label="Ruilin Luo">Ruilin Luo<sup>1</sup></a>,
                </span>
                <span class="author-block">
                  <span class="author-block">
                    <span class="author-name">Shiyang Huang<sup>2</sup></span>,
                  </span>
                </span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=dbCdFjoAAAAJ" target="_blank" rel="noopener">Zongqi Wang<sup>1</sup></a>,
                </span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=4gH3sxsAAAAJ" target="_blank" rel="noopener">Yujiu Yang<sup>1</sup></a>,
                </span>
                <span class="author-block">
                  <span class="author-block">
                    <span class="author-name">Yan Teng<sup>2</sup></span>,
                  </span>
                </span>
                <span class="author-block">
                  <span class="author-block">
                    <span class="author-name">Yingchun Wang<sup>2</sup></span>,
                  </span>
                </span>
              </div>

              <div class="publication-affiliations">
                <span><sup>1</sup>Tsinghua University</span><br>
                <span><sup>2</sup>Shanghai Artificial Intelligence Laboratory</span><br>
                <span><sup>3</sup>Fudan University</span><br>
              </div>
              

              <div class="column has-text-centered">
                <div class="publication-links">
                  <span class="link-block">
                    <a href="https://YOUR_DOMAIN.com/static/pdfs/paper.pdf" target="_blank" rel="noopener"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon"><i class="fas fa-file-pdf"></i></span>
                      <span>Paper</span>
                    </a>
                  </span>

                  <span class="link-block">
                    <a href="https://github.com/Carol-gutianle/UniMod" target="_blank" rel="noopener"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon"><i class="fab fa-github"></i></span>
                      <span>Code</span>
                    </a>
                  </span>

                  <span class="link-block">
                    <a href="https://huggingface.co/collections/Carol0110/unimod" target="_blank" rel="noopener"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg"
                             alt="Hugging Face"
                             style="height: 1.25em; vertical-align: middle;">
                      </span>
                      <span>Datasets & Models</span>
                    </a>
                  </span>

                  <span class="link-block">
                    <a href="https://arxiv.org/abs/ARXIV_ID_HERE" target="_blank" rel="noopener"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon"><i class="ai ai-arxiv"></i></span>
                      <span>arXiv</span>
                    </a>
                  </span>
                </div>
              </div>

            </div><!-- column -->
          </div><!-- columns -->
        </div><!-- container -->
      </div><!-- hero-body -->
    </section>

  
  <!-- Image carousel -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          <!-- TODO: Replace with your research result images -->
          <img src="static/images/unimod_overview.jpg" alt="Overview of the UniMod framework." loading="lazy"/>
          <!-- TODO: Replace with description of this result -->
          <h2 class="subtitle has-text-centered">
            The left panel illustrates the comparison between UniMod and traditional baselines, where UniMod introduces a structured reasoning trajectory comprising Evidence, Modality, Risk, Policy, and Answer.
    The center panel, UniTrace, demonstrates the consensus mechanism used to select specialized teacher models (e.g., GLM) for labeling each trajectory node.
    The right panel details the Training stage, where the UniMod is optimized via UniRM. UniRM utilizes a shared VLM backbone with task-specific heads, incorporating head-wise weight subspace decoupling and stochastic head scheduling.
          </h2>
        </div>
        <div class="item">
          <!-- Your image here -->
          <img src="static/images/unimod_result.png" alt="Comparison of moderation models on UniTrace, text-based and vision-based benchmarks." loading="lazy"/>
          <h2 class="subtitle has-text-centered">
            VLM-based models are ranked by an overall score (mean of Text and Image Avg), indicated by blue shading where deeper intensity signifies a higher ranking.
            UniMod achieves the best overall performance while using substantially fewer
            training samples than prior high-performing VLM-based guards.
            Best results are shown in bold, and second-best are underlined.         
          </h2>
        </div>
        <div class="item">
          <!-- Your image here -->
          <img src="static/images/unirm_result.png" alt="Comparison of reward models on UniReward and RewardBench-2." loading="lazy"/>
          <h2 class="subtitle has-text-centered">
          Best results are shown in bold, and second-best are underlined.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/ablation_1.jpg" alt="Performance comparison of UniMod against various ablation variants." loading="lazy"/>
        <h2 class="subtitle has-text-centered">
          The first three panels illustrate the training dynamics for Formality, Modality and Risk attributes. The final panel shows the downstream F1 scores for both text and image moderation.        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/ablation_2.jpg" alt="Ablation study of UniRM and scalability of UniMod." loading="lazy"/>
        <h2 class="subtitle has-text-centered">
          (a-b) Average performance and variance of the UniRM under various ablation settings. (c) Data Scaling: F1 score improvement of UniMod when training data is scaled from $L_1$ to $L_2$. (d) Model Scaling: Comparison of F1 score gains ($\Delta$) across different moderation models when increasing model capacity from 3B to 7B parameters.      </div>
    </div>
  </div>
  </div>
  </section>
  <!-- End image carousel -->




    <!-- Paper abstract -->
    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                Safety moderation is pivotal for identifying harmful content.
                Despite the success of textual safety moderation, its multimodal counterparts remain hindered by a dual sparsity of data and supervision.
                Conventional reliance on binary labels lead to shortcut learning, which obscures the intrinsic classification boundaries necessary for effective multimodal discrimination.
                Hence, we propose a novel learning paradigm (UniMod) that transitions from sparse decision-making to dense reasoning traces.
                By constructing structured trajectories encompassing evidence grounding, modality assessment, risk mapping, policy decision, and response generation, we reformulate monolithic decision tasks into a multi-dimensional boundary learning process.
                This approach forces the model to ground its decision in explicit safety semantics, preventing the model from converging on superficial shortcuts.
                To facilitate this paradigm, we develop a multi-head scalar reward model (UniRM).
                UniRM provides multi-dimensional supervision by assigning attribute-level scores to the response generation stage.
                Furthermore, we introduce specialized optimization strategies to decouple task-specific parameters and rebalance training dynamics, effectively resolving interference between diverse objectives in multi-task learning.
                Empirical results show UniMod achieves competitive textual moderation performance and sets a new multimodal state-of-the-art (SOTA) using less than 40% of the training data used by leading baselines.
                Ablations further validate our multi-attribute trajectory reasoning, offering an effective and efficient framework for multimodal moderation.              
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- (rest of your template continues unchanged...) -->

  </main>

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank" rel="noopener">Academic Project Page Template</a>
              adopted from the <a href="https://nerfies.github.io" target="_blank" rel="noopener">Nerfies</a> project page.
              You are free to borrow the source code of this website; we just ask that you link back to this page in the footer. <br>
              This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank" rel="noopener">
              Creative Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>
</html>
